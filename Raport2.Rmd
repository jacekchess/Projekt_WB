---
title: "Comparision of efficiency of various data imputation techniques in R"
author: "Agata Makarewicz, Martyna Majchrzak, Jacek Wiśniewski"
date: "27 04 2020"
output: html_document
---

```{r setup, include=TRUE, warning=FALSE, echo=FALSE, resuls='hide', include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, cache=TRUE)
library(tidyverse)
library(reshape2)
library(dplyr)
library(xgboost)
library(kknn)
library(purrr)
source('imputations_performance.R')
source('load_datasets.R')
```

## Abstract

Imputation of missing values is a common step in the machine learning process and sometimes a difficult problem. Many real-life datasets contain incomplete observations and dealing with them is a key part of modeling as most of the algorithms provided by widely used R packages (for instance caret or mlr) require complete data.
This report aims to measure the influence of five different imputation methods on the performance of a selected classification model. Simple, common methods such as basic mean, median, and mode are compared with advanced imputation techniques from specialized R packages - mice, VIM, and softImpute.
As tested algorithms, Recursive Partitioning And Regression Trees, Naive Bayes, Ranger (Random forest),  Linear Discriminant Analysis, and k-Nearest Neighbours were chosen.
Its prediction effectiveness is assessed by F1 measure (also called F1 score) to provide a proper measure for both balanced and imbalanced data.\

## Introduction 

Dealing with missing values is an important part of reprocessing data used for machine learning models because they do not accept incomplete observations.
Many methods allow handling this issue. 
Firstly, there are basic ones such as removing rows or imputing with mean or mode, which are fast and easy to implement, however they do not guarantee a very good performance. On the other hand, many sophisticated strategies exist, e.g. using tree-based models, which usually result in better models, but enhance the algorithm's complexity and computational time.
In this paper, the quality of 5 different approaches to data imputation is compared, and their impact on binary classification data modeling is assessed.
Summary of the results presents which imputations contributed to achieving the best model's predictions. 
To achieve the goal of this study the following experiment will be conducted on 11 datasets, gathered mostly from OpenML and some from the imputation packages.

## Methodology 

In the following table are presented the datasets used for this experiment, along with their OpenML ID number, name, number of instances, number of features, and number of missing values. The datasets vary in size and number of missing values.


```{r dataset table, results='hide'}
library(dplyr)
data_nr <- c("1018","1590", "188", "23381", "27", "29", "38", "4", "40536", "41278", "55", "56", "6332", "944")
data_name <- c("ipums_la_99-small", "adult", "eucalyptus", "dresses-sales", "colic", "credit-approval", "sick", "labor", "SpeedDating", "stem-okcupid", "hepatitis", "vote", "cylinder-bands", "echoMonths" )
nr_instances <- c(8844, 48842, 736, 500, 368, 690, 3772, 57, 8378, 45907, 155, 435, 540, 130)
nr_features <- c(56, 13, 16,  13, 20, 16, 28, 17, 123, 20, 20, 17, 34, 10)
nr_missing<-c(34843, 6465, 455, 955, 1199, 67, 2293, 326, 18570,139693, 167, 392, 999, 97)

datasets <- data.frame(cbind(data_nr, 
                     data_name, 
                     nr_instances, 
                     nr_features,
                     nr_missing))

datasets1<-datasets%>%
  transmute(data_nr=data_nr,
            data_name=data_name,
            nr_instances=as.numeric(as.character(nr_instances)),
            nr_features=as.numeric(as.character(nr_features)),
            nr_missing=as.numeric(as.character(nr_missing)))%>%
  mutate(procent_missing=round(nr_missing/(nr_instances*nr_features),2))

```

```{r tab1, resuls='hide'}
knitr::kable(datasets1, caption = "Tab.1. Datasets")
```

### Imputation strategies

The imputations, that were performed and analyzed include:

* **mean/mode imputation**\
  One of the basic techniques replaces missing values with a mean (for continuous variables) and mode (for categorical variables) of complete values in the given variable. Implemented with basic R functions.

* **mice (predictive mean matching)**\
   Performs multivariate imputation by chained equations, meaning it creates multiple imputations (replacement values) for multivariate missing data. Implemented with mice() function (with method parameter set to "pmm") from mice package.

* **k-nearest neighbours**\
  An aggregation of the non-missing values of the k nearest neighbors is used as an imputed value. The kind of aggregation depends on the type of the variable. Implemented with kNN() function from the VIM package.
  
* **hotdeck** \
  Each missing value is replaced with an observed response from a “similar” unit. Implemented with hotdeck() function from VIM package.

* **softImpute combined with median/mode imputation**\
  For numeric variables function softImpute() from softImpute package is used, fitting a low-rank matrix approximation to a matrix with missing values via nuclear-norm regularization. For remaining variables, missing values are imputed with median or mode, which is implemented with impute() function from the imputeMissings() package.
  
### Classification algorithms

Five different algorithms were chosen, two linear classifiers (NB, LDA), two tree-based models (RPART, Ranger), and one kernel estimation (KNN) were tested after every imputation on every dataset. The modeling was performed using the mlr3 package.

* **RPART (Recursive Partitioning And Regression Trees)**\
  Based on decision trees, works by splitting the dataset recursively, until a predetermined termination criterion is reached or no improvement can be made.  At each step, the split is made based on the independent variable that splits the data best (results in the largest possible reduction in the heterogeneity of the dependent (predicted) variable).
  
* **NB (Naive Bayes)**\
  Technique based on an assumption of independence between predictors (every pair of features being classified is independent of each other) and their equal contribution to the outcome, then the probability of an event occurring given the probability of another event that has already occurred is calculated.
  
* **Ranger (Random forest)**\
  Constructs a large number of individual decision trees that operate as an ensemble, each tree returns a class prediction and the class that is the mode of the classes (in case of classification) of the individual trees becomes our model’s prediction.
  
* **LDA (Linear Discriminant Analysis)**\
  Based on the dimension reduction technique, it finds a new feature space to project the data to maximize class separability. More precisely, it finds a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination is used as a linear classifier, which uses Bayes’ Theorem to estimate probabilities of belonging to respective classes.

* **KNN (k-Nearest Neighbour)**\
  An object is classified by a majority vote of its neighbors, being assigned to the class most common among its k nearest neighbors. The neighbors are measured by a distance function.

### Implementation

To conduct the experiment two functions were written:

 * **save_files()** - takes a dataset, divides it into the train and test datasets, imputes them in 5 ways described above and saves the imputed datasets into files

Dataset is split into train set (80% of observation) and train set (20% of observation).
They are imputed separately, using the methods described above, to avoid data leakage.

 * **imputations_performance()** - takes dataset name and classification algorithm returns performance of each dataset imputation combined with this algorithm

The performance is checked using **F1 Measure** -  the harmonic mean of the precision and recall.
 
 $$F_1=2*\frac{precision*recall}{precision+recall}$$
 
Due to that measure, a ranking of imputations for each dataset is created. Imputations are given ranks from 1 to 5:

 * if imputations have the same F1 measure, they receive the same rank
 
 * if imputation method failed to impute the data, it receives rank 5
 
The overall measure of the imputation performance is a mean of its ranks from all datasets.
 

## Results

### Results Rpart

```{r small data, results='hide'}
evaluation_944_rpart <- imputations_performance(target944, "dataset944", "classif.rpart")
evaluation_56_rpart <- imputations_performance(target56, "dataset56", "classif.rpart") 
evaluation_55_rpart <- imputations_performance(target55,  "dataset55", "classif.rpart")
evaluation_38_rpart <- imputations_performance(target38,  "dataset38", "classif.rpart") 
evaluation_27_rpart <- imputations_performance(target27,  "dataset27", "classif.rpart")
evaluation_188_rpart <- imputations_performance(target188, "dataset188", "classif.rpart") 
evaluation_29_rpart <- imputations_performance(target29, "dataset29", "classif.rpart") 
evaluation_4_rpart <- imputations_performance(target4,  "dataset4", "classif.rpart")
evaluation_1018_rpart <- imputations_performance(target1018, "dataset1018", "classif.rpart")
evaluation_23381_rpart <- imputations_performance(target23381, "dataset23381", "classif.rpart")
evaluation_40536_rpart <- imputations_performance(target40536,  "dataset40536", "classif.rpart")
evaluation_6332_rpart <- imputations_performance(target6332, "dataset6332", "classif.rpart")
evaluation_1590_rpart <- imputations_performance(target1590, "dataset1590", "classif.rpart")
evaluation_41278_rpart <- imputations_performance(target41278,"dataset41278", "classif.rpart") 
```


```{r f1 rpart, results= 'markup'}
# 1590, 188, 27, 29, 38, 4, 40536, 55, 56, 6332, 944
f1_combined_rpart<-rbind(
  evaluation_1018_rpart,
  evaluation_1590_rpart,
  evaluation_188_rpart,
  evaluation_23381_rpart,
  evaluation_27_rpart,
  evaluation_29_rpart,
  evaluation_38_rpart,
  evaluation_4_rpart,
  evaluation_40536_rpart,
  evaluation_41278_rpart,
  evaluation_55_rpart,
  evaluation_56_rpart,
  evaluation_6332_rpart,
  evaluation_944_rpart
)
rownames(f1_combined_rpart) <- NULL
colnames(f1_combined_rpart)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')
f1_combined_rpart<-cbind(data_name, f1_combined_rpart)

```

```{r write_rpart}
write.csv2(f1_combined_rpart, "./results_csv/rpart.csv", row.names = FALSE)
```

```{r tab2, resuls='hide'}
knitr::kable(f1_combined_rpart, caption = "Tab.2. Rpart results")
```

```{r old rating function}

# ranking<-function(f1_combined){
#   
# 
#   df <- transpose(f1_combined[, -1], keep.names = "variable")
#   mean <- colMeans(f1_combined[, -1])
#   min <- apply(f1_combined[, -1], 2, min)
#   max <- apply(f1_combined[, -1], 2, max)
#   df <- cbind(df, mean, min, max)
#   
#   df <- df %>%
#     mutate(rank_min = dense_rank(min)) %>%
#     mutate(rank_mean = dense_rank(mean)) %>%
#     mutate(rank_max = dense_rank(max)) %>%
#     mutate(value = rowSums(.[16:18])/15)
#   
#   df_ranks<-df%>%
#     arrange(desc(value))%>%
#     cbind(ranks=c(1,2,3,4,5))%>%
#     select(variable, ranks)
#   
#   df<-df%>%
#     inner_join(df_ranks, by=c("variable"="variable"))
#   
#   return(df)
# }


```

```{r ranking function}

ranking<-function(f1_combined){
  data_names<-as.character(f1_combined$data_name)
  df <- transpose(f1_combined[, -1], keep.names = "imputation")
  ranks<-as.data.frame(lapply(df%>%select(-imputation), dense_rank))%>%
    mutate(imputation=df$imputation)
  ranks[is.na(ranks)]<-5
  n<-ncol(ranks)
  ranks<-ranks%>%select(n, 1:(n-1))
  colnames(ranks)<-c("imputation",data_names)
  
  ranks<-cbind(ranks, mean=rowMeans(ranks[2:n]))
  
  return(ranks)
}

```

### Ranking Rpart

```{r rating rpart, results='hide'}

ranking_rpart<-ranking(f1_combined_rpart)
means_rpart <- ranking_rpart%>%select(imputation, mean)
ranking_rpart_melt<-melt(ranking_rpart%>%select(-mean))

ggplot(ranking_rpart_melt, aes(x = imputation, y = value)) +
  geom_boxplot()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  stat_summary(fun.y=mean, colour="darkred", geom="point",
               shape=18, size=3,show_guide = FALSE) +
  geom_text(data = means_rpart, aes(label = round(mean, 2), y = mean + 0.1))
```

```{r write_ranking_rpart}
write.csv2(ranking_rpart, "./results_csv/ranking_rpart.csv", row.names = FALSE)
```

### Results Naive Bayes

```{r nb 1, results='hide'}

evaluation_944_nb <- imputations_performance(target944, "dataset944", "classif.naive_bayes")
evaluation_56_nb <- imputations_performance(target56, "dataset56", "classif.naive_bayes") 
evaluation_55_nb <- imputations_performance(target55,  "dataset55", "classif.naive_bayes")
evaluation_38_nb <- imputations_performance(target38,  "dataset38", "classif.naive_bayes")  
evaluation_27_nb <- imputations_performance(target27,  "dataset27", "classif.naive_bayes")
evaluation_188_nb <- imputations_performance(target188, "dataset188", "classif.naive_bayes") 
evaluation_29_nb <- imputations_performance(target29, "dataset29", "classif.naive_bayes") 
evaluation_4_nb <- imputations_performance(target4,  "dataset4", "classif.naive_bayes")
evaluation_1018_nb <-imputations_performance(target1018,"dataset1018","classif.naive_bayes")
evaluation_23381_nb <-imputations_performance(target23381,"dataset23381","classif.naive_bayes")
evaluation_40536_nb <- imputations_performance(target40536,  "dataset40536", "classif.naive_bayes")
evaluation_6332_nb <- imputations_performance(target6332, "dataset6332", "classif.naive_bayes") 
evaluation_1590_nb <- imputations_performance(target1590, "dataset1590", "classif.naive_bayes") 
evaluation_41278_nb <- imputations_performance(target41278, "dataset41278", "classif.naive_bayes") 
```



```{r f1 nb, results= 'markup'}
f1_combined_nb<-as.data.frame(
  rbind(
  evaluation_1018_nb,
  evaluation_1590_nb,
  evaluation_188_nb,
  evaluation_23381_nb,
  evaluation_27_nb,
  evaluation_29_nb,
  evaluation_38_nb,
  evaluation_4_nb,
  evaluation_40536_nb,
  evaluation_41278_nb,
  evaluation_55_nb,
  evaluation_56_nb,
  evaluation_6332_nb,
  evaluation_944_nb
))
rownames(f1_combined_nb) <- NULL
colnames(f1_combined_nb)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')
f1_combined_nb<-cbind(data_name, f1_combined_nb)

```

```{r write_nb}
write.csv2(f1_combined_nb, "./results_csv/nb.csv", row.names = FALSE)
```

```{r tab3, resuls='hide'}
knitr::kable(f1_combined_nb, caption = "Tab.3. Naive Bayes results")
```

### Ranking Naive Bayes

```{r rating nb, results='hide'}
ranking_nb<-ranking(f1_combined_nb)
means_nb <- ranking_nb%>%select(imputation, mean)
ranking_nb_melt<-melt(ranking_nb%>%select(-mean))

ggplot(ranking_nb_melt, aes(x = imputation, y = value)) +
  geom_boxplot()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  stat_summary(fun.y=mean, colour="darkred", geom="point",
               shape=18, size=3,show_guide = FALSE) +
  geom_text(data = means_nb, aes(label = round(mean, 2), y = mean + 0.1))
```

```{r write_ranking_nb}
write.csv2(ranking_nb, "./results_csv/ranking_nb.csv", row.names = FALSE)
```

### Results Ranger 

```{r ranger 1, results='hide'}

evaluation_944_ranger <- imputations_performance(target944, "dataset944", "classif.ranger")   
evaluation_56_ranger <- imputations_performance(target56, "dataset56", "classif.ranger") 
evaluation_55_ranger <- imputations_performance(target55,  "dataset55", "classif.ranger")  
evaluation_38_ranger <- imputations_performance(target38,  "dataset38", "classif.ranger")  
evaluation_27_ranger <- imputations_performance(target27,  "dataset27", "classif.ranger") 
evaluation_188_ranger <- imputations_performance(target188, "dataset188", "classif.ranger") 
evaluation_29_ranger <- imputations_performance(target29, "dataset29", "classif.ranger")
evaluation_4_ranger <- imputations_performance(target4,  "dataset4", "classif.ranger")
evaluation_1018_ranger <- imputations_performance(target1018,"dataset1018","classif.ranger")
evaluation_23381_ranger <- imputations_performance(target23381,"dataset23381","classif.ranger")
evaluation_40536_ranger <- imputations_performance(target40536,  "dataset40536", "classif.ranger")
evaluation_6332_ranger <- imputations_performance(target6332, "dataset6332", "classif.ranger") 
evaluation_1590_ranger <- imputations_performance(target1590, "dataset1590", "classif.ranger") 
evaluation_41278_ranger <- imputations_performance(target41278, "dataset41278", "classif.ranger") 
```



```{r f1 ranger, results= 'markup'}
f1_combined_ranger<-as.data.frame(
  rbind(
  evaluation_1018_ranger,
  evaluation_1590_ranger,
  evaluation_188_ranger,
  evaluation_23381_ranger,
  evaluation_27_ranger,
  evaluation_29_ranger,
  evaluation_38_ranger,
  evaluation_4_ranger,
  evaluation_40536_ranger,
  evaluation_41278_ranger,
  evaluation_55_ranger,
  evaluation_56_ranger,
  evaluation_6332_ranger,
  evaluation_944_ranger
))
rownames(f1_combined_ranger) <- NULL
colnames(f1_combined_ranger)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')
f1_combined_ranger<-cbind(data_name, f1_combined_ranger)

```

```{r write_ranger}
write.csv2(f1_combined_ranger, "./results_csv/ranger.csv", row.names = FALSE)
```

```{r tab4, resuls='hide'}
knitr::kable(f1_combined_ranger, caption = "Tab.4. Ranger results")
```

### Rating Ranger

```{r rating ranger, results='hide'}
ranking_ranger<-ranking(f1_combined_ranger)
means_ranger <- ranking_ranger%>%select(imputation, mean)
ranking_ranger_melt<-melt(ranking_ranger%>%select(-mean))

ggplot(ranking_ranger_melt, aes(x = imputation, y = value)) +
  geom_boxplot()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  stat_summary(fun.y=mean, colour="darkred", geom="point",
               shape=18, size=3,show_guide = FALSE) +
  geom_text(data = means_ranger, aes(label = round(mean, 2), y = mean + 0.1))
```

```{r write_ranking_ranger}
write.csv2(ranking_ranger, "./results_csv/ranking_ranger.csv", row.names = FALSE)
```

### Results LDA 

```{r lda 1, results='hide'}

evaluation_944_lda <- imputations_performance(target944, "dataset944", "classif.lda")   
evaluation_56_lda <- imputations_performance(target56, "dataset56", "classif.lda") 
evaluation_55_lda <- imputations_performance(target55,  "dataset55", "classif.lda")  
evaluation_38_lda <- imputations_performance(target38,  "dataset38", "classif.lda")  
evaluation_27_lda <- imputations_performance(target27,  "dataset27", "classif.lda") 
evaluation_188_lda <- imputations_performance(target188, "dataset188", "classif.lda")
evaluation_29_lda <- imputations_performance(target29, "dataset29", "classif.lda") 
evaluation_4_lda <- imputations_performance(target4,  "dataset4", "classif.lda")
evaluation_1018_lda <- imputations_performance(target1018, "dataset1018", "classif.lda")
evaluation_23381_lda <- imputations_performance(target23381, "dataset23381", "classif.lda")
evaluation_40536_lda <- imputations_performance(target40536,  "dataset40536", "classif.lda")
evaluation_6332_lda <- imputations_performance(target6332, "dataset6332", "classif.lda")
evaluation_1590_lda <- imputations_performance(target1590, "dataset1590", "classif.lda") 
evaluation_41278_lda <- imputations_performance(target41278, "dataset41278", "classif.lda") 
```


```{r f1 lda, results= 'markup'}
f1_combined_lda<-as.data.frame(
  rbind(
  evaluation_1018_lda,
  evaluation_1590_lda,
  evaluation_188_lda,
  evaluation_23381_lda,
  evaluation_27_lda,
  evaluation_29_lda,
  evaluation_38_lda,
  evaluation_4_lda,
  evaluation_40536_lda,
  evaluation_41278_lda,
  evaluation_55_lda,
  evaluation_56_lda,
  evaluation_6332_lda,
  evaluation_944_lda
))
rownames(f1_combined_lda) <- NULL
colnames(f1_combined_lda)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')
f1_combined_lda<-cbind(data_name, f1_combined_lda)

```

```{r write_lda}
write.csv2(f1_combined_lda, "./results_csv/lda.csv", row.names = FALSE)
```

```{r tab5, resuls='hide'}
knitr::kable(f1_combined_lda, caption = "Tab.5. LDA results")
```

### Rating LDA

```{r rating lda, results='hide'}
ranking_lda<-ranking(f1_combined_lda)
means_lda <- ranking_lda%>%select(imputation, mean)
ranking_lda_melt<-melt(ranking_lda%>%select(-mean))

ggplot(ranking_lda_melt, aes(x = imputation, y = value)) +
  geom_boxplot()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  stat_summary(fun.y=mean, colour="darkred", geom="point",
               shape=18, size=3,show_guide = FALSE) +
  geom_text(data = means_lda, aes(label = round(mean, 2), y = mean + 0.1))
```

```{r write_ranking_lda}
write.csv2(ranking_lda, "./results_csv/ranking_dla.csv", row.names = FALSE)
```

### Results KNN 

```{r kknn 1, results='hide'}

evaluation_944_kknn <- imputations_performance(target944, "dataset944", "classif.kknn") 
evaluation_56_kknn <- imputations_performance(target56, "dataset56", "classif.kknn") 
evaluation_55_kknn <- imputations_performance(target55,  "dataset55", "classif.kknn")  
evaluation_38_kknn <- imputations_performance(target38,  "dataset38", "classif.kknn")  
evaluation_27_kknn <- imputations_performance(target27,  "dataset27", "classif.kknn")  
evaluation_188_kknn <- imputations_performance(target188, "dataset188", "classif.kknn") 
evaluation_29_kknn <- imputations_performance(target29, "dataset29", "classif.kknn")
evaluation_4_kknn <- imputations_performance(target4,  "dataset4", "classif.kknn")  
evaluation_1018_kknn <- imputations_performance(target1018, "dataset1018", "classif.kknn")
evaluation_23381_kknn <- imputations_performance(target23381, "dataset23381", "classif.kknn")
evaluation_40536_kknn <- imputations_performance(target40536,  "dataset40536", "classif.kknn") 
evaluation_6332_kknn <- imputations_performance(target6332, "dataset6332", "classif.kknn") 
evaluation_1590_kknn <- imputations_performance(target1590, "dataset1590", "classif.kknn")
evaluation_41278_kknn <- imputations_performance(target41278, "dataset41278", "classif.kknn") 
```


```{r f1 kknn, results= 'markup'}
# 1590, 188, 27, 29, 38, 4, 40536, 55, 56, 6332, 944
f1_combined_kknn<-as.data.frame(
  rbind(
  evaluation_1018_kknn,
  evaluation_1590_kknn,
  evaluation_188_kknn,
  evaluation_23381_kknn,
  evaluation_27_kknn,
  evaluation_29_kknn,
  evaluation_38_kknn,
  evaluation_4_kknn,
  evaluation_40536_kknn,
  evaluation_41278_kknn,
  evaluation_55_kknn,
  evaluation_56_kknn,
  evaluation_6332_kknn,
  evaluation_944_kknn
))
rownames(f1_combined_kknn) <- NULL
colnames(f1_combined_kknn)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')
f1_combined_kknn<-cbind(data_name, f1_combined_kknn)

```

```{r write_kknn}
write.csv2(f1_combined_kknn, "./results_csv/kknn.csv", row.names = FALSE)
```

```{r tab6, resuls='hide'}
knitr::kable(f1_combined_kknn, caption = "Tab.6. KNN results")
```

###  Rating KKNN

```{r rating kknn, results='hide'}
ranking_kknn<-ranking(f1_combined_kknn)
means_kknn <- ranking_kknn%>%select(imputation, mean)
ranking_kknn_melt<-melt(ranking_kknn%>%select(-mean))

ggplot(ranking_kknn_melt, aes(x = imputation, y = value)) +
  geom_boxplot()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  stat_summary(fun.y=mean, colour="darkred", geom="point",
               shape=18, size=3,show_guide = FALSE) +
  geom_text(data = means_kknn, aes(label = round(mean, 2), y = mean + 0.1))
```

```{r write_ranking_kknn}
write.csv2(ranking_kknn, "./results_csv/ranking_kknn.csv", row.names = FALSE)
```
## Rating summary

```{r rating summary}

ranking_combined<-as.data.frame(cbind(
  rpart=ranking_rpart$mean,
  nb=ranking_nb$mean,
  ranger=ranking_ranger$mean,
  lda=ranking_lda$mean,
  kknn=ranking_kknn$mean))
row.names(ranking_combined)<-ranking_rpart$imputation



heatmap(as.matrix(ranking_combined))

```

## Conclusions

```{r}
f1_point_data <- melt(f1_combined_rpart)

ggplot(f1_point_data, aes(x = data_name, y = value, color = variable)) +
  geom_jitter(width=0.15) +
  theme(axis.text.x = element_text(angle = 45))+
  scale_x_discrete(expand=c(0.1, 0.2))
```

## Numer of missing values

Taking only datasets with a high number of missing into consideration.

```{r missing set}
missing<-datasets1%>%select(data_nr,data_name, procent_missing)%>%
  filter(data_nr %in% c(23381, 27,4,41278))
knitr::kable(missing, caption = "Tab.7. Datasets with most missing values")
```

```{r missing vis}
ranking_rpart_mis<-ranking(f1_combined_rpart%>%filter(data_name %in% c('dresses-sales', 'colic', 'labor', 'stem-okcupid')))
ranking_lda_mis<-ranking(f1_combined_lda%>%filter(data_name %in% c('dresses-sales', 'colic', 'labor', 'stem-okcupid')))
ranking_ranger_mis<-ranking(f1_combined_ranger%>%filter(data_name %in% c('dresses-sales', 'colic', 'labor', 'stem-okcupid')))
ranking_nb_mis<-ranking(f1_combined_nb%>%filter(data_name %in% c('dresses-sales', 'colic', 'labor', 'stem-okcupid')))
ranking_kknn_mis<-ranking(f1_combined_kknn%>%filter(data_name %in% c('dresses-sales', 'colic', 'labor', 'stem-okcupid')))


ranking_combined_mis<-as.data.frame(cbind(
  rpart=ranking_rpart_mis$mean,
  nb=ranking_nb_mis$mean,
  ranger=ranking_ranger_mis$mean,
  lda=ranking_lda_mis$mean,
  kknn=ranking_kknn_mis$mean))
row.names(ranking_combined_mis)<-c('insert_mean','mice_pmm','vim_knn','vim_hotdeck','softImpute')

heatmap(as.matrix(ranking_combined_mis))
```



## References 

1. mice package https://cran.r-project.org/web/packages/mice/index.html

2. VIM package https://cran.r-project.org/web/packages/VIM/index.html

3. softImpute package https://cran.r-project.org/web/packages/softImpute/index.html

4. mlr3 package https://mlr3.mlr-org.com/